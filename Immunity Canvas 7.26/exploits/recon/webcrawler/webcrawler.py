#!/usr/bin/env python
##ImmunityHeader v1
###############################################################################
## File       :  webcrawler.py
## Description:
##            :
## Created_On :  Tue Oct 17 2017
## Created_By :  X.
##
## (c) Copyright 2015, Immunity, Inc. all rights reserved.
###############################################################################

import os
import re
import sys
import copy
import socket
import logging

if '.' not in sys.path:
    sys.path.append('.')

NAME                            = 'WebCrawler - URI discovery tool'
DESCRIPTION                     = 'Attempt to find all the possible URI on a given website.'
DOCUMENTATION                   = {}
DOCUMENTATION['VENDOR']         = ''
DOCUMENTATION['Date public']    = ''
DOCUMENTATION['Repeatability']  = ''

VERSION                         = '1.0'
PROPERTY                        = {}
PROPERTY['TYPE']                = 'Recon'
PROPERTY['SITE']                = 'Remote'

CHANGELOG = """
"""

NOTES = """

The module starts with the root and parses the answer, potentially adding more
URI to send requests to. When the module has completed its task, the module
records the information with the knowledge database:
    - When the module is called again, the knowledge is fetched directly (cache
      effect) therefore the module terminates instantly.
    - Other modules will have access to this knowledge (URI) and may use it.
    - The knowledge is basically a list of (presumably) valid URI for a given
      vhost/port/protocol (http or https) accessible at the target address.

Note: The user may force the cache to be updated if required by checking the
corresponding option on the GUI or adding -Oflushcache on the CLI.

Tested against immunityinc.com, infiltratecon.com.

To execute a command:
python exploits/recon/webcrawler/webcrawler.py -t $IP -p 80 -Ovhost:www2.foobar.com
"""


from canvasexploit import canvasexploit
from exploitutils import *
import canvasengine
import libs.spkproxy as spkproxy

PATTERN1 = '<(img|script)[\s]+[a-zA-Z-_=\s"]*src=\"(.*?)\"'
PATTERN2 = '<(a|div|link)[\s]*(.*?)href=\"(.*?)\"'
PATTERN3 = 'url[(](.*?)[)]'

USUAL_BLACKLIST = ['.mov', '.js', '.pkg', '.jpg', '.png', '.pdf', '.zip', '.exe', '.tgz', '.tar', '.gif']

class Uri(dict):

    def __init__(self, ip, path, method='GET', vhost='', ssl=True, exploit_class=None):
        dict.__init__(self)
        self['ip'] = ip
        self['vhost'] = vhost
        self['path'] = path
        self['method'] = method
        self['response'] = None
        self['ssl'] = ssl
        self.exploit_class = exploit_class

    def __str__(self):
        if self['response']:
            status = self['response']['code']
        else:
            status = '???'
        return "%s %s [%s]" % (self['method'], self.get_url(), status)

    def get_path(self):
        return self['path']

    def get_url(self):
        url = "https" if self['ssl'] else "http"
        url += "://%s" % (self['vhost'] if self['vhost'] else self['ip'])
        url += self['path']
        return url

    def get_base_dir(self):
        s = self.get_path()
        base_dir = '/' + '/'.join(s.replace('//','/').replace('//','/').split('/')[1:-1:])
        if base_dir != '/':
            base_dir += '/'
        return base_dir

    def get_http_code(self):
        if not self['response']:
            return -1
        return self['response']['code']

    def get_http_payload(self):
        if not self['response']:
            return None
        return self['response']['payload'].read()

    def get_method(self):
        return self['method']

    def set_method(self, method):
        self['method'] = method
        self['response'] = None

    def send_request(self):
        url = self.get_url()

        entireresponse = True
        if self.href_is_blacklisted(self['path']):
            entireresponse=False

        try:
            (payload, http_code) = spkproxy.urlopen(url,
                                                exploit=None,
                                                entireresponse=entireresponse,
                                                return_response_code=True,
                                                auth=None,
                                                verb=self['method'])
        except Exception as e:
            logging.error("Error: %s" % str(e))
            return

        self['response'] = {'code': http_code, 'payload': payload }


    def href_is_blacklisted(self, path):

        current_blacklist = USUAL_BLACKLIST
        for extension in current_blacklist:
            if path.split('?')[0].endswith(extension):
                return True
        return False

    def href_is_email(self, path):
        if 'mailto:' in path:
            return True
        return False

    def href_is_full_url(self, path):
        if '://' in path:
            return True
        return False

    def extract_from_url(self, url):
        http_base = 'http://%s/' % self['vhost']
        https_base = 'https://%s/' % self['vhost']
        if url.startswith(http_base) or url.startswith(https_base):
            L1 = url.split(http_base)
            L2 = url.split(https_base)
            if len(L1) == 2:
                return '/' + L1[1]
            if len(L2) == 2:
                return '/' + L2[1]
        return ''

    def __extract_from_img_scripts(self, payload, base_dir):
        """
        e.g. <img src="images/innuendoLogo.png" alt="" style="width:70px;height:87px;">
        e.g. <script src="js/bin/dependencies.min.js?v=_pf14526145367449"></script>
        """

        res = []
        patterns = re.findall(PATTERN1, payload)
        if not patterns:
            return res

        # For each pattern matched, we extract the ref.
        # The ref is either relative (what we are looking for)
        # Or absolute (full URL, which we dismiss for now)
        for pattern in patterns:
            uri_candidate = str(pattern[1])

            # Skip full URL for now as there are too many things to check
            if self.href_is_email(uri_candidate) or self.href_is_full_url(uri_candidate):
                continue

            # The candidate must begin with /
            if uri_candidate[0] != '/':
                tmp = os.path.abspath(os.path.join(base_dir, uri_candidate))
                uri_candidate = tmp

            res.append(uri_candidate)
        return res

    def __extract_from_anchor(self, payload, base_dir):
        """
        e.g. <link rel="stylesheet" href="css/min.css?v=_pf14526145367449">
        e.g. <a class="canvas_tutorial" href="../../x/y/php_demo.swf">
        e.g. <a href="education/web-hacking.html">&gt;&gt; learn more</a>
        """
        res = []
        patterns = re.findall(PATTERN2, payload)
        if not patterns:
            return res

        # For each pattern matched, we extract the ref.
        # The ref is either relative (what we are looking for)
        # or absolute (which needs a special treatment)
        for pattern in patterns:
            uri_candidate = str(pattern[2])

            # Skip full URL for now as there are too many things to check
            if self.href_is_email(uri_candidate):
                continue

            if self.href_is_full_url(uri_candidate):
                uri_candidate = self.extract_from_url(uri_candidate)

            # Skip empty patterns
            if not uri_candidate:
                continue

            # The candidate must begin with /
            if uri_candidate[0] != '/':
                tmp = os.path.abspath(os.path.join(base_dir, uri_candidate))
                uri_candidate = tmp

            res.append(uri_candidate)

        return res

    def __extract_from_css(self, payload, base_dir):
        """
        e.g. [...]);src:url(fonts/lato-regular-webfont.eot?#iefix) format('em[...]
        """
        res = []

        if not '.css' in self['path'] and not '.js' in self['path']:
            return res

        print self['path']

        patterns = re.findall(PATTERN3, payload)
        if not patterns:
            return res

        print patterns

        # For each pattern matched, we extract the ref.
        # The ref is either relative (what we are looking for)
        # or absolute (which needs a special treatment)
        for pattern in patterns:
            uri_candidate = str(pattern)

            # Skip full URL for now as there are too many things to check
            if self.href_is_email(uri_candidate):
                continue

            if self.href_is_full_url(uri_candidate):
                uri_candidate = self.extract_from_url(uri_candidate)

            # Skip empty patterns
            if not uri_candidate:
                continue

            # The candidate must begin with /
            if uri_candidate[0] != '/':
                tmp = os.path.abspath(os.path.join(base_dir, uri_candidate))
                uri_candidate = tmp

            res.append(uri_candidate)

        return res

    def uri_extract(self):
        """
        e.g. <img src=\"something/foo.png\"
        e.g. <script src=\"something/foo.png\"
        """

        res = []
        base_dir = self.get_base_dir()

        # Extract all the subdirs
        dir_list = base_dir.split('/')[1:-1:]
        s = '/'
        for _dir in dir_list:
            s += _dir + '/'
            res.append(s)

        # Do we have a payload?
        payload = self.get_http_payload()
        if not payload:
            return list(set(res))

        # If we do, can extract find interesting fields?
        res += self.__extract_from_img_scripts(payload, base_dir)
        res += self.__extract_from_anchor(payload, base_dir)
        res += self.__extract_from_css(payload, base_dir)

        filtered_res = list(set(res))
        for x in filtered_res:
            logging.debug("Potential candidate: %s" % x)

        return filtered_res


class UriStack(list):
    """
    Class object used to store URI, their results etc...
    It is a stack so that we can pop and push objects for processing.
    """
    def __init__(self, ip, vhost='', ssl=True):
        list.__init__(self)
        self.ip = ip
        self.vhost = vhost
        self.ssl = ssl
        # adding the first element which is always the root path
        #self.append(ip, "/", 'GET', vhost=self.vhost, ssl=self.ssl)
        self.append(ip, "/", 'GET', vhost=self.vhost, ssl=self.ssl)


    def append(self, ip, path, method='GET', vhost='', ssl=True):
        uriObj = Uri(ip, path, method=method,  vhost=vhost, ssl=ssl)
        if not self.find_uri_object(uriObj):
            list.append(self, uriObj)

    def append_object(self, obj):
        if not self.find_uri_object(obj):
            list.append(self, obj)

    def pop(self):
        for uri in self:
            if uri['response'] is None:
                return uri
        return None

    def find_uri_object(self, uriObj):
        for u in self:
            # Should add more checks later (hostname etc...)
            if u['path'] == uriObj['path'] and u['method'] == uriObj['method'] and u['vhost'] == uriObj['vhost']:
                return True
        return False

    def add_list(self, uri_list, method='GET'):

        for uri in uri_list:
            # First we add the URI itself
            self.append(self.ip, uri, method=method, vhost=self.vhost, ssl=self.ssl)


class theexploit(canvasexploit):

    def __init__(self):
        canvasexploit.__init__(self)

        self.name       = NAME
        self.port       = 80
        self.ssl        = False
        self.vhost      = ""
        self.uri_db     = None
        self.debug      = False
        self.flushcache = False

    def is_alive(self):
        """
        Detects if the connection can be established or not.
        """
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.connect((self.host,self.port))
            s.close()
            return True
        except Exception as e:
            return False

    def getArgs(self):
        """
        Get arguments for attack
        """
        self.host       = self.target.interface
        self.port       = int(self.argsDict.get("port", self.port))
        self.vhost      = self.argsDict.get("vhost", self.vhost)
        self.ssl        = self.argsDict.get("ssl", self.ssl)
        self.debug      = self.argsDict.get("debug", self.debug)
        self.flushcache = self.argsDict.get("flushcache", self.flushcache)

        if not self.vhost:
            self.vhost = str(self.host)

        # TODO: authentication
        return

    def get_url(self, vhost, path):
        """
        Creates an URL based on the vhost, the port and the path (URI).
        """
        url = "https" if self.ssl else "http"
        url += "://%s:%s" % (vhost, self.port)
        url += path
        return url

    def do_spiding(self, db, base_url):
        """
        Performs the actual crawling process.
        """
        self.uri_db = UriStack(self.host, vhost=self.vhost, ssl=self.ssl)
        # While loop to perform all the queued requests.
        # This is guarantied to end as we do not add twice an URI and once all
        # the requests have been completed there is no object left to pop.
        uri_candidate = self.uri_db.pop()
        while uri_candidate:

            logging.info("current request: %s" % uri_candidate)
            uri_candidate.send_request()
            http_code = uri_candidate.get_http_code()
            method = uri_candidate.get_method()

            if method == 'GET' and http_code == 200:
                uri_list = uri_candidate.uri_extract()
                self.uri_db.add_list(uri_list)

            if method == 'GET' and  http_code in [301, 302]:
                # TODO: Temporary patch.
                # We need to parse completely the Location
                path = uri_candidate.get_path()
                self.uri_db.add_list([path + '/'])

            uri_candidate = self.uri_db.pop()

        db[base_url] = {'vhost': self.vhost, 'port': self.port, 'ssl': self.ssl, 'uri': {}}
        nbr_records = 0
        for uri in self.uri_db:
            http_code = uri.get_http_code()
            if http_code not in [400, 404]:
                if not db[base_url]['uri'].has_key(http_code):
                    db[base_url]['uri'][http_code] = []
                db[base_url]['uri'][http_code].append(uri.get_path())
                nbr_records += 1

        return nbr_records

    def display_uri(self, db, base_url):
        """
        Simple function that displays the URI found on the console.
        """
        for http_code in db[base_url]['uri']:
            for uri in db[base_url]['uri'][http_code]:
                logging.info('Found %s' % uri)

    def flush_uri_cache(self):
        """
        Flush the URI cache.
        This is not done automatically and is triggered by the user.
        """
        logging.info('Flushing the cache')
        if self.target.get_knowledge("URI"):
            self.target.replace_knowledge("URI", {}, 100)

    def run(self):
        """
        Runs the attempt to grab information
        """
        self.getArgs()
        self.setInfo("Running %s against %s:%s [%s]" % (NAME, self.host, self.port, self.vhost))

        # We may need more debugging
        if self.debug:
            logger = logging.getLogger()
            logger.setLevel(logging.DEBUG)

        if not self.is_alive():
            logging.error('The connection can not be established.')
            logging.error("%s - found no reachable host!" % self.name)
            self.setInfo("%s - found no reachable host!" % self.name)
            return 0

        if self.flushcache:
            self.flush_uri_cache()

        # Throws a warning if it is likely that the user should have checked the SSL box
        if not self.ssl and self.port in [443, 8443]:
            logging.warning('You are running the webcrawler against port %d yet SSL/TLS is disabled' % self.port)
        if self.ssl and self.port in [80, 8080]:
            logging.warning('You are running the webcrawler against port %d yet SSL/TLS is enabled' % self.port)


        base_url = self.get_url(self.vhost, '/')
        logging.info('Webcrawling the URL %s' % base_url)

        vh = self.target.get_knowledge('URI')
        if vh:
            db = vh.known
            if vh.known.has_key(base_url):
                logging.warning('We already have the information for this URL in cache')
                self.display_uri(db, base_url)
                self.setInfo("%s for %s - done (success)" % (NAME, self.host))
                return 1
        else:
            db = {}

        nbr_records = self.do_spiding(db, base_url)
        if nbr_records:
            logging.info("%d total records added for URL %s" % (nbr_records, base_url))
            self.display_uri(db, base_url)
            self.target.add_knowledge("URI", db, 100)
            self.setInfo("%s for %s - done (success)" % (NAME, self.host))
            return 1

        logging.error("%s - found no reachable URI!" % self.name)
        self.setInfo("%s - found no reachable URI!" % self.name)
        return 0

if __name__ == '__main__':
    app = theexploit()
    ret = standard_callback_commandline(app)
